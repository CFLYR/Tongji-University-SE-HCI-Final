 
"""
ç»Ÿä¸€PPTæ‰‹åŠ¿è¯†åˆ«æ’­æ”¾å™¨
Unified PPT Gesture Recognition Controller

åŠŸèƒ½ç‰¹æ€§:
1. åŸºç¡€æ’­æ”¾æ§åˆ¶ (ä¸Šä¸€é¡µ/ä¸‹ä¸€é¡µ/æš‚åœ/é€€å‡º)
2. é«˜çº§äº¤äº’åŠŸèƒ½ (æ¿€å…‰æŒ‡ç¤ºå™¨/ç”»ç¬”/ç¼©æ”¾)
3. è‡ªå®šä¹‰æ‰‹åŠ¿é…ç½®
4. å¤šç§æ‰‹åŠ¿è¯†åˆ«ç®—æ³•
5. å®æ—¶åé¦ˆå’ŒçŠ¶æ€æ˜¾ç¤º
6. æ”¯æŒå¤šç§PPTè½¯ä»¶
"""

import cv2 as cv
import numpy as np
import time
import math
import json
import os
import pyautogui as pt
from enum import Enum
from dataclasses import dataclass, asdict
from typing import Dict, List, Tuple, Optional, Callable
import handTrackingModule as hmt
from chinese_text_renderer import put_chinese_text, ChineseTextRenderer, put_text_auto
from ppt_controller import PPTController, get_ppt_controller
from speech_text_manager import SpeechTextManager, SpeechScrollDisplay


class GestureType(Enum):
    """æ‰‹åŠ¿ç±»å‹æšä¸¾"""
    STATIC = "static"  # é™æ€æ‰‹åŠ¿ (æ‰‹æŒ‡å½¢çŠ¶)
    DYNAMIC = "dynamic"  # åŠ¨æ€æ‰‹åŠ¿ (è¿åŠ¨è½¨è¿¹)
    DUAL_HAND = "dual_hand"  # åŒæ‰‹æ‰‹åŠ¿
    CONTINUOUS = "continuous"  # æŒç»­æ‰‹åŠ¿


class PPTAction(Enum):
    """PPTæ“ä½œæšä¸¾"""
    NEXT_SLIDE = "next_slide"
    PREV_SLIDE = "prev_slide"
    PLAY_PAUSE = "play_pause"
    EXIT_PRESENTATION = "exit_presentation"
    FULLSCREEN_TOGGLE = "fullscreen_toggle"
    DRAW_MODE = "draw_mode"
    ERASE_MODE = "erase_mode"
    ZOOM_IN = "zoom_in"
    ZOOM_OUT = "zoom_out"
    JUMP_TO_PAGE = "jump_to_page"
    MENU_TOGGLE = "menu_toggle"
    SPEECH_SCROLL_TOGGLE = "speech_scroll_toggle"  # åˆ‡æ¢æ¼”è®²ç¨¿æ»šåŠ¨æ˜¾ç¤º
    SPEECH_NEXT = "speech_next"  # æ¼”è®²ç¨¿ä¸‹ä¸€æ®µ
    SPEECH_PREV = "speech_prev"  # æ¼”è®²ç¨¿ä¸Šä¸€æ®µ


@dataclass
class GestureConfig:
    """æ‰‹åŠ¿é…ç½®æ•°æ®ç±»"""
    name: str
    gesture_type: GestureType
    action: PPTAction
    finger_pattern: List[int] = None  # é™æ€æ‰‹åŠ¿çš„æ‰‹æŒ‡æ¨¡å¼ [æ‹‡æŒ‡,é£ŸæŒ‡,ä¸­æŒ‡,æ— åæŒ‡,å°æŒ‡]
    motion_pattern: str = None  # åŠ¨æ€æ‰‹åŠ¿çš„è¿åŠ¨æ¨¡å¼
    confidence_threshold: float = 0.8  # è¯†åˆ«ç½®ä¿¡åº¦é˜ˆå€¼
    hold_duration: float = 0.0  # æŒç»­æ—¶é—´è¦æ±‚ (ç§’)
    enabled: bool = True  # æ˜¯å¦å¯ç”¨
    custom: bool = False  # æ˜¯å¦ä¸ºè‡ªå®šä¹‰æ‰‹åŠ¿


class UnifiedGestureDetector:
    """ç»Ÿä¸€æ‰‹åŠ¿è¯†åˆ«å™¨"""

    def __init__(self):
        self.detector = hmt.handDetector()
        self.tipIds = [4, 8, 12, 16, 20]  # æ‰‹æŒ‡å°–ç«¯ID

        # æ‰‹åŠ¿å†å²è®°å½• (ç”¨äºåŠ¨æ€æ‰‹åŠ¿å’ŒæŒç»­æ‰‹åŠ¿)
        self.gesture_history = []
        self.position_history = []
        self.last_gesture_time = {}

        # åŒæ‰‹æ£€æµ‹
        self.left_hand_landmarks = None
        self.right_hand_landmarks = None

    def detect_static_gesture(self, lmList: List[List[int]]) -> Dict[str, float]:
        """æ£€æµ‹é™æ€æ‰‹åŠ¿ - è¿”å›å„ç§æ‰‹åŠ¿çš„ç½®ä¿¡åº¦"""
        if len(lmList) == 0:
            return {}

        fingers = self.get_fingers_up(lmList)
        total_fingers = sum(fingers)

        gestures = {}

        # åŸºç¡€æ•°å­—æ‰‹åŠ¿
        if total_fingers == 0:
            gestures["fist"] = 1.0
        elif total_fingers == 1:
            if fingers[1] == 1:  # é£ŸæŒ‡
                gestures["point"] = 1.0
            elif fingers[0] == 1:  # æ‹‡æŒ‡
                gestures["thumb_up"] = 1.0
        elif total_fingers == 2:
            if fingers[1] == 1 and fingers[2] == 1:
                gestures["peace"] = 1.0
            elif fingers[0] == 1 and fingers[1] == 1:
                # æ£€æŸ¥OKæ‰‹åŠ¿
                distance = self.get_distance(4, 8, lmList)
                if distance < 40:
                    gestures["ok"] = 0.9
                else:
                    gestures["two"] = 0.8
        elif total_fingers == 3:
            gestures["three"] = 1.0
        elif total_fingers == 4:
            gestures["four"] = 1.0
        elif total_fingers == 5:
            gestures["open_hand"] = 1.0

        # ç‰¹æ®Šæ‰‹åŠ¿
        if fingers[1] == 1 and fingers[4] == 1 and sum(fingers[2:4]) == 0:
            gestures["rock"] = 0.9

        return gestures

    def detect_dynamic_gesture(self, lmList: List[List[int]]) -> Dict[str, float]:
        """æ£€æµ‹åŠ¨æ€æ‰‹åŠ¿ - åŸºäºè¿åŠ¨è½¨è¿¹"""
        if len(lmList) == 0:
            return {}

        # è®°å½•æ‰‹æŒä¸­å¿ƒä½ç½®
        hand_center = lmList[9]  # æ‰‹æŒä¸­å¿ƒ
        self.position_history.append([hand_center[1], hand_center[2], time.time()])

        # ä¿æŒæœ€è¿‘30å¸§çš„å†å²
        if len(self.position_history) > 30:
            self.position_history = self.position_history[-30:]

        gestures = {}

        if len(self.position_history) >= 10:
            # åˆ†æè¿åŠ¨æ¨¡å¼
            recent_positions = self.position_history[-10:]

            # è®¡ç®—æ€»ä½“è¿åŠ¨æ–¹å‘
            start_pos = recent_positions[0]
            end_pos = recent_positions[-1]
            dx = end_pos[0] - start_pos[0]
            dy = end_pos[1] - start_pos[1]
            distance = math.sqrt(dx * dx + dy * dy)

            if distance > 50:  # æœ‰æ˜æ˜¾ç§»åŠ¨
                # æ°´å¹³ç§»åŠ¨
                if abs(dx) > abs(dy) * 2:
                    if dx > 0:
                        gestures["swipe_right"] = min(distance / 100, 1.0)
                    else:
                        gestures["swipe_left"] = min(distance / 100, 1.0)
                # å‚ç›´ç§»åŠ¨
                elif abs(dy) > abs(dx) * 2:
                    if dy > 0:
                        gestures["swipe_down"] = min(distance / 100, 1.0)
                    else:
                        gestures["swipe_up"] = min(distance / 100, 1.0)

                # æ£€æµ‹åœ†å½¢è¿åŠ¨
                if self.is_circular_motion(recent_positions):
                    gestures["circle"] = 0.8

        return gestures

    def detect_dual_hand_gesture(self, left_landmarks, right_landmarks) -> Dict[str, float]:
        """æ£€æµ‹åŒæ‰‹æ‰‹åŠ¿"""
        gestures = {}

        if left_landmarks is not None and right_landmarks is not None:
            # è®¡ç®—åŒæ‰‹è·ç¦»
            left_center = left_landmarks[9]
            right_center = right_landmarks[9]

            distance = self.get_distance_between_points(
                left_center[1], left_center[2],
                right_center[1], right_center[2]
            )

            # åŒæ‰‹å¼ å¼€/åˆæ‹¢
            if distance > 200:
                gestures["hands_spread"] = min(distance / 300, 1.0)
            elif distance < 100:
                gestures["hands_together"] = 1.0 - (distance / 100)

            # åŒæ‰‹æ‹³å¤´
            left_fingers = self.get_fingers_up(left_landmarks)
            right_fingers = self.get_fingers_up(right_landmarks)

            if sum(left_fingers) == 0 and sum(right_fingers) == 0:
                gestures["double_fist"] = 1.0

        return gestures

    def get_fingers_up(self, lmList: List[List[int]]) -> List[int]:
        """æ£€æµ‹ç«–èµ·çš„æ‰‹æŒ‡"""
        fingers = []

        if len(lmList) == 0:
            return fingers

        # æ‹‡æŒ‡
        if lmList[self.tipIds[0]][1] > lmList[self.tipIds[0] - 1][1]:
            fingers.append(1)
        else:
            fingers.append(0)

        # å…¶ä»–å››æ ¹æ‰‹æŒ‡
        for id in range(1, 5):
            if lmList[self.tipIds[id]][2] < lmList[self.tipIds[id] - 2][2]:
                fingers.append(1)
            else:
                fingers.append(0)

        return fingers

    def get_distance(self, p1: int, p2: int, lmList: List[List[int]]) -> float:
        """è®¡ç®—ä¸¤ç‚¹é—´è·ç¦»"""
        if len(lmList) == 0:
            return 0
        x1, y1 = lmList[p1][1], lmList[p1][2]
        x2, y2 = lmList[p2][1], lmList[p2][2]
        return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)

    def get_distance_between_points(self, x1: int, y1: int, x2: int, y2: int) -> float:
        """è®¡ç®—ä»»æ„ä¸¤ç‚¹é—´è·ç¦»"""
        return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)

    def is_circular_motion(self, positions: List[List]) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºåœ†å½¢è¿åŠ¨"""
        if len(positions) < 8:
            return False

        # ç®€å•çš„åœ†å½¢æ£€æµ‹ç®—æ³•
        center_x = sum(pos[0] for pos in positions) / len(positions)
        center_y = sum(pos[1] for pos in positions) / len(positions)

        # è®¡ç®—æ¯ä¸ªç‚¹åˆ°ä¸­å¿ƒçš„è·ç¦»å˜åŒ–
        distances = []
        for pos in positions:
            dist = math.sqrt((pos[0] - center_x) ** 2 + (pos[1] - center_y) ** 2)
            distances.append(dist)

        # å¦‚æœè·ç¦»å˜åŒ–ä¸å¤§ï¼Œå¯èƒ½æ˜¯åœ†å½¢
        avg_dist = sum(distances) / len(distances)
        variance = sum((d - avg_dist) ** 2 for d in distances) / len(distances)

        return variance < (avg_dist * 0.3) ** 2  # è·ç¦»å˜åŒ–å°äº30%


class UnifiedPPTGestureController:
    """ç»Ÿä¸€PPTæ‰‹åŠ¿è¯†åˆ«æ’­æ”¾å™¨ä¸»ç±»"""

    def __init__(self, config_file: str = "gesture_config.json"):
        self.gesture_detector = UnifiedGestureDetector()
        self.ppt_controller = PPTController()
        self.config_file = config_file

        # åˆå§‹åŒ–ä¸­æ–‡æ–‡æœ¬æ¸²æŸ“å™¨
        self.chinese_renderer = ChineseTextRenderer()

        # åˆå§‹åŒ–æ¼”è®²ç¨¿ç®¡ç†å™¨
        self.speech_manager = SpeechTextManager()
        self.speech_display = None
        self.show_speech_scroll = False

        # åŠ è½½æ‰‹åŠ¿é…ç½®
        self.gesture_configs = self.load_gesture_configs()

        # çŠ¶æ€å˜é‡
        self.running = True
        self.show_help = False
        self.calibration_mode = False

        # æ€§èƒ½ç›‘æ§
        self.fps = 0
        self.frame_count = 0
        self.start_time = time.time()

        # æ¿€å…‰æŒ‡ç¤ºå™¨ç›¸å…³
        self.laser_point = None
        self.draw_trail = []

        # å‘½ä»¤æ‰§è¡Œå†·å´æ§åˆ¶
        self.command_cooldown_duration = 2.0  # æ‰§è¡Œå‘½ä»¤åå†·å´2ç§’
        self.last_command_execution_time = 0
        
        # çª—å£åç§°ï¼Œä½¿ç”¨è‹±æ–‡é¿å…ä¸­æ–‡ä¹±ç 
        self.window_name = 'Gesture Recognition'

        # å°è¯•è‡ªåŠ¨åˆå§‹åŒ–PPT
       #self.auto_initialize_ppt()

    def auto_initialize_ppt(self):
        """è‡ªåŠ¨åˆå§‹åŒ–PPTæ¼”ç¤º"""
        try:
            print(" æ­£åœ¨æœç´¢PPTæ–‡ä»¶...")
            ppt_file = self.ppt_controller.auto_select_ppt()

            if ppt_file:
                print(f" æ‰¾åˆ°PPTæ–‡ä»¶: {os.path.basename(ppt_file)}")
                user_input = input("æ˜¯å¦è¦è‡ªåŠ¨æ‰“å¼€PPTæ¼”ç¤ºï¼Ÿ(y/n): ").lower().strip()

                if user_input in ['y', 'yes', 'æ˜¯', '']:
                    if self.ppt_controller.open_powerpoint_file(ppt_file):
                        print(" PPTæ¼”ç¤ºå·²å¯åŠ¨ï¼Œå¯ä»¥å¼€å§‹æ‰‹åŠ¿æ§åˆ¶")
                    else:
                        print(" PPTå¯åŠ¨å¤±è´¥ï¼Œå°†åªæ˜¾ç¤ºæ‰‹åŠ¿æ£€æµ‹")
                else:
                    print(" è·³è¿‡PPTè‡ªåŠ¨å¯åŠ¨ï¼Œæ‰‹åŠ¨å¯åŠ¨PPTåå¯ä½¿ç”¨æ‰‹åŠ¿æ§åˆ¶")
            else:
                print(" å½“å‰ç›®å½•æœªæ‰¾åˆ°PPTæ–‡ä»¶")
                print(" æç¤ºï¼šå°†PPTæ–‡ä»¶æ”¾åœ¨ç¨‹åºç›®å½•ä¸‹ï¼Œæˆ–æ‰‹åŠ¨æ‰“å¼€PPTåä½¿ç”¨æ‰‹åŠ¿æ§åˆ¶")

        except Exception as e:
            print(f" PPTåˆå§‹åŒ–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            print(" ç»§ç»­è¿è¡Œç¨‹åºï¼Œå¯æ‰‹åŠ¨æ‰“å¼€PPTä½¿ç”¨æ‰‹åŠ¿æ§åˆ¶")

    def load_gesture_configs(self) -> Dict[str, GestureConfig]:
        """åŠ è½½æ‰‹åŠ¿é…ç½®"""
        default_configs = {
            "next_slide": GestureConfig(
                name="ä¸‹ä¸€é¡µ",
                gesture_type=GestureType.DYNAMIC,
                action=PPTAction.NEXT_SLIDE,
                motion_pattern="swipe_right",
                confidence_threshold=0.7
            ),
            "prev_slide": GestureConfig(
                name="ä¸Šä¸€é¡µ",
                gesture_type=GestureType.DYNAMIC,
                action=PPTAction.PREV_SLIDE,
                motion_pattern="swipe_left",
                confidence_threshold=0.7
            ),
            "pause": GestureConfig(
                name="æš‚åœ/æ’­æ”¾",
                gesture_type=GestureType.STATIC,
                action=PPTAction.PLAY_PAUSE,
                finger_pattern=[0, 0, 0, 0, 0],  # æ‹³å¤´
                confidence_threshold=0.9,
                enabled=False  # ç¦ç”¨æš‚åœ/æ’­æ”¾æ‰‹åŠ¿
            ),            "exit": GestureConfig(
                name="é€€å‡º",
                gesture_type=GestureType.DUAL_HAND,
                action=PPTAction.EXIT_PRESENTATION,
                confidence_threshold=0.8,
                hold_duration=2.0
            ),
            "fullscreen": GestureConfig(
                name="å…¨å±åˆ‡æ¢",
                gesture_type=GestureType.STATIC,
                action=PPTAction.FULLSCREEN_TOGGLE,
                finger_pattern=[1, 1, 1, 1, 1],  # å¼ å¼€æ‰‹æŒ
                confidence_threshold=0.9,
                hold_duration=1.0,
                enabled=False  # ç¦ç”¨å…¨å±åˆ‡æ¢æ‰‹åŠ¿
            ),
            "speech_scroll": GestureConfig(
                name="æ¼”è®²ç¨¿æ»šåŠ¨",
                gesture_type=GestureType.STATIC,
                action=PPTAction.SPEECH_SCROLL_TOGGLE,
                finger_pattern=[1, 1, 0, 0, 0],  # æ‹‡æŒ‡+é£ŸæŒ‡ (OKæ‰‹åŠ¿)
                confidence_threshold=0.8,
                hold_duration=1.5,
                enabled=True
            ),
            "speech_next": GestureConfig(
                name="æ¼”è®²ç¨¿ä¸‹ä¸€æ®µ",
                gesture_type=GestureType.DYNAMIC,
                action=PPTAction.SPEECH_NEXT,
                motion_pattern="swipe_up",
                confidence_threshold=0.7,
                enabled=True
            ),
            "speech_prev": GestureConfig(
                name="æ¼”è®²ç¨¿ä¸Šä¸€æ®µ",
                gesture_type=GestureType.DYNAMIC,
                action=PPTAction.SPEECH_PREV,
                motion_pattern="swipe_down",
                confidence_threshold=0.7,
                enabled=True
            )
        }

        if os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    configs = {}
                    for key, value in data.items():
                        # å¤„ç†æšä¸¾ç±»å‹è½¬æ¢
                        if 'gesture_type' in value and isinstance(value['gesture_type'], str):
                            value['gesture_type'] = GestureType(value['gesture_type'])
                        if 'action' in value and isinstance(value['action'], str):
                            value['action'] = PPTAction(value['action'])
                        configs[key] = GestureConfig(**value)
                    return configs
            except Exception as e:
                print(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")

        return default_configs

    def save_gesture_configs(self):
        """ä¿å­˜æ‰‹åŠ¿é…ç½®åˆ°æ–‡ä»¶"""
        try:
            configs_dict = {}
            for key, config in self.gesture_configs.items():
                # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
                config_dict = asdict(config)
                # å¤„ç†æšä¸¾ç±»å‹
                if hasattr(config_dict['gesture_type'], 'value'):
                    config_dict['gesture_type'] = config_dict['gesture_type'].value
                if hasattr(config_dict['action'], 'value'):
                    config_dict['action'] = config_dict['action'].value
                configs_dict[key] = config_dict
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(configs_dict, f, ensure_ascii=False, indent=2)
            print(f" é…ç½®å·²ä¿å­˜åˆ°: {self.config_file}")
        except Exception as e:
            print(" ä¿å­˜é…ç½®å¤±è´¥:", e)

    def process_frame(self, img):
        """å¤„ç†è§†é¢‘å¸§"""
        # ç¿»è½¬å›¾åƒ (é•œåƒæ•ˆæœ)
        img = cv.flip(img, 1)

        # æ£€æµ‹æ‰‹éƒ¨
        img = self.gesture_detector.detector.findHands(img)
        lmList = self.gesture_detector.detector.findPosition(img, draw=False)

        # è¯†åˆ«æ‰‹åŠ¿
        detected_gestures = {}
        current_time = time.time()

        if len(lmList) != 0:
            # é™æ€æ‰‹åŠ¿æ£€æµ‹
            static_gestures = self.gesture_detector.detect_static_gesture(lmList)
            detected_gestures.update(static_gestures)

            # åŠ¨æ€æ‰‹åŠ¿æ£€æµ‹
            dynamic_gestures = self.gesture_detector.detect_dynamic_gesture(lmList)
            detected_gestures.update(dynamic_gestures)

            # æ¿€å…‰æŒ‡ç¤ºå™¨åŠŸèƒ½ (å®æ—¶æ›´æ–°ï¼Œä¸å—å†·å´é™åˆ¶)
            if self.ppt_controller.laser_mode:
                index_tip = lmList[8]  # é£ŸæŒ‡å°–
                self.laser_point = (index_tip[1], index_tip[2])

        # åŒæ‰‹æ‰‹åŠ¿æ£€æµ‹ (éœ€è¦æ›´å¤æ‚çš„å®ç°)
        # dual_gestures = self.gesture_detector.detect_dual_hand_gesture(left_lm, right_lm)
        # detected_gestures.update(dual_gestures)

        # æ£€æŸ¥å‘½ä»¤æ‰§è¡Œå†·å´çŠ¶æ€
        current_time = time.time()
        in_cooldown = (current_time - self.last_command_execution_time) < self.command_cooldown_duration

        # å®æ—¶è¿›è¡Œæ‰‹åŠ¿æ£€æµ‹ï¼ˆä¸å—å†·å´å½±å“ï¼‰
        if detected_gestures:
            self.match_and_execute_gestures(detected_gestures, current_time, in_cooldown)

        # ç»˜åˆ¶ç•Œé¢å…ƒç´ 
        self.draw_ui(img, detected_gestures, lmList)

        return img

    def match_and_execute_gestures(self, detected_gestures: Dict[str, float], current_time: float, in_cooldown: bool):
        """åŒ¹é…æ£€æµ‹åˆ°çš„æ‰‹åŠ¿å¹¶æ‰§è¡Œç›¸åº”æ“ä½œ"""

        # å¦‚æœåœ¨å†·å´æœŸå†…ï¼Œä¸æ‰§è¡Œä»»ä½•å‘½ä»¤ï¼Œåªè¿›è¡Œæ‰‹åŠ¿æ£€æµ‹æ˜¾ç¤º
        if in_cooldown:
            return

        for config_key, config in self.gesture_configs.items():
            if not config.enabled:
                continue

            matched = False
            confidence = 0.0

            # æ ¹æ®æ‰‹åŠ¿ç±»å‹è¿›è¡ŒåŒ¹é…
            if config.gesture_type == GestureType.STATIC:
                # é™æ€æ‰‹åŠ¿åŒ¹é…
                for gesture_name, gesture_confidence in detected_gestures.items():
                    if self.matches_static_pattern(gesture_name, config):
                        confidence = gesture_confidence
                        matched = True
                        break

            elif config.gesture_type == GestureType.DYNAMIC:
                # åŠ¨æ€æ‰‹åŠ¿åŒ¹é…
                if config.motion_pattern in detected_gestures:
                    confidence = detected_gestures[config.motion_pattern]
                    matched = True            # æ£€æŸ¥ç½®ä¿¡åº¦é˜ˆå€¼
            if matched and confidence >= config.confidence_threshold:
                # æ£€æŸ¥æŒç»­æ—¶é—´è¦æ±‚
                if config.hold_duration > 0:
                    if config_key not in self.gesture_detector.last_gesture_time:
                        self.gesture_detector.last_gesture_time[config_key] = current_time
                    elif current_time - self.gesture_detector.last_gesture_time[config_key] >= config.hold_duration:
                        self.execute_custom_action(config.action)
                        self.gesture_detector.last_gesture_time[config_key] = current_time
                        # è®°å½•å‘½ä»¤æ‰§è¡Œæ—¶é—´ï¼Œå¼€å§‹å†·å´
                        self.last_command_execution_time = current_time
                        print(f" æ‰§è¡Œå‘½ä»¤: {config.name}, å†·å´{self.command_cooldown_duration}ç§’")
                else:
                    # é˜²æ­¢é‡å¤è§¦å‘
                    if config_key not in self.gesture_detector.last_gesture_time or \
                            current_time - self.gesture_detector.last_gesture_time[config_key] > 1.0:
                        self.execute_custom_action(config.action)
                        self.gesture_detector.last_gesture_time[config_key] = current_time
                        # è®°å½•å‘½ä»¤æ‰§è¡Œæ—¶é—´ï¼Œå¼€å§‹å†·å´
                        self.last_command_execution_time = current_time
                        print(f" æ‰§è¡Œå‘½ä»¤: {config.name}, å†·å´{self.command_cooldown_duration}ç§’")
            else:
                # é‡ç½®æŒç»­æ—¶é—´è®¡æ—¶å™¨
                if config_key in self.gesture_detector.last_gesture_time:
                    del self.gesture_detector.last_gesture_time[config_key]

    def matches_static_pattern(self, gesture_name: str, config: GestureConfig) -> bool:
        """æ£€æŸ¥é™æ€æ‰‹åŠ¿æ˜¯å¦åŒ¹é…é…ç½®æ¨¡å¼"""
        # ç®€åŒ–çš„åŒ¹é…é€»è¾‘ï¼Œå®é™…å¯ä»¥æ›´å¤æ‚
        pattern_map = {
            "fist": [0, 0, 0, 0, 0],
            "point": [0, 1, 0, 0, 0],
            "thumb_up": [1, 0, 0, 0, 0],
            "peace": [0, 1, 1, 0, 0],
            "ok": [1, 1, 0, 0, 0],
            "three": [0, 1, 1, 1, 0],
            "four": [0, 1, 1, 1, 1],
            "open_hand": [1, 1, 1, 1, 1]
        }

        if gesture_name in pattern_map and config.finger_pattern:
            return pattern_map[gesture_name] == config.finger_pattern
        return False

    def draw_ui(self, img, detected_gestures: Dict[str, float], lmList):
        """ç»˜åˆ¶ç”¨æˆ·ç•Œé¢"""
        h, w, c = img.shape

        # ç»˜åˆ¶FPS
        cv.putText(img, f"FPS: {int(self.fps)}", (10, 30),
                   cv.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

        # ç»˜åˆ¶PPTçŠ¶æ€ä¿¡æ¯
        ppt_status = "æ¼”ç¤ºä¸­" if self.ppt_controller.is_presentation_active else "å¾…æœº"
        status_text = f"PPTçŠ¶æ€: {ppt_status}"
        img = put_text_auto(img, status_text, (10, 60), 18, (0, 255, 255))

        # ç»˜åˆ¶å½“å‰PPTæ–‡ä»¶ä¿¡æ¯
        if self.ppt_controller.current_ppt_path:
            ppt_name = os.path.basename(self.ppt_controller.current_ppt_path)
            ppt_info = f"æ–‡ä»¶: {ppt_name}"
            img = put_text_auto(img, ppt_info, (10, 90), 18, (255, 255, 255))

        # ç»˜åˆ¶å‘½ä»¤å†·å´çŠ¶æ€ä¿¡æ¯
        current_time = time.time()
        time_since_execution = current_time - self.last_command_execution_time
        cooldown_remaining = max(0, self.command_cooldown_duration - time_since_execution)

        if cooldown_remaining > 0:
            cooldown_text = f"å‘½ä»¤å†·å´ä¸­: {cooldown_remaining:.1f}ç§’ (å†·å´æ—¶é•¿: {self.command_cooldown_duration}ç§’)"
            color = (0, 165, 255)  # æ©™è‰²è¡¨ç¤ºå†·å´ä¸­
        else:
            cooldown_text = f"å¯æ‰§è¡Œå‘½ä»¤ (å†·å´æ—¶é•¿: {self.command_cooldown_duration}ç§’)"
            color = (0, 255, 0)  # ç»¿è‰²è¡¨ç¤ºå¯æ‰§è¡Œ

        img = put_text_auto(img, cooldown_text, (10, 120), 16, color)

        # ç»˜åˆ¶æ£€æµ‹åˆ°çš„æ‰‹åŠ¿
        y_offset = 150
        for gesture_name, confidence in detected_gestures.items():
            if confidence > 0.5:  # åªæ˜¾ç¤ºé«˜ç½®ä¿¡åº¦çš„æ‰‹åŠ¿
                text = f"{gesture_name}: {confidence:.2f}"
                cv.putText(img, text, (10, y_offset),
                           cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
                y_offset += 25

        # ç»˜åˆ¶æ¿€å…‰æŒ‡ç¤ºå™¨
        if self.laser_point and self.ppt_controller.is_presentation_active:
            cv.circle(img, self.laser_point, 10, (0, 0, 255), -1)
            cv.circle(img, self.laser_point, 15, (255, 255, 255), 2)

        # ç»˜åˆ¶æ‰‹éƒ¨å…³é”®ç‚¹ (å¦‚æœéœ€è¦)
        if len(lmList) != 0 and self.calibration_mode:
            for lm in lmList:
                cv.circle(img, (lm[1], lm[2]), 5, (255, 0, 255), -1)        # ç»˜åˆ¶å¸®åŠ©ä¿¡æ¯
        if self.show_help:
            self.draw_help_overlay(img)

    def draw_help_overlay(self, img):
        """ç»˜åˆ¶å¸®åŠ©è¦†ç›–å±‚"""
        h, w = img.shape[:2]
        overlay = np.zeros((h, w, 3), dtype=np.uint8)
        overlay[:] = (0, 0, 0)  # é»‘è‰²èƒŒæ™¯

        help_text = [
            "æ‰‹åŠ¿æ§åˆ¶å¸®åŠ©:",
            "å³æ»‘ - ä¸‹ä¸€é¡µ",
            "å·¦æ»‘ - ä¸Šä¸€é¡µ",
            "åŒæ‹³(é•¿æŒ‰) - é€€å‡º",
            "",
            "ç³»ç»Ÿç‰¹æ€§:",
            "âœ“ å®æ—¶æ‰‹åŠ¿æ£€æµ‹",
            "âœ“ å‘½ä»¤æ‰§è¡Œåå†·å´é˜²è¯¯è§¦",
            "",
            "æŒ‰é”®æ§åˆ¶:",
            "H - æ˜¾ç¤º/éšè—å¸®åŠ©",
            "C - æ ¡å‡†æ¨¡å¼",
            "S - ä¿å­˜é…ç½®",
            "T - æ–‡æœ¬è¾“å…¥åŒ¹é…æ¨¡å¼",
            "N - æ¼”è®²ç¨¿ä¸‹ä¸€æ®µ",
            "P - æ¼”è®²ç¨¿ä¸Šä¸€æ®µ",
            "+ - å¢åŠ å†·å´æ—¶é—´",
            "- - å‡å°‘å†·å´æ—¶é—´",
            "Q/ESC - é€€å‡ºç¨‹åº"
        ]

        y = 50
        for text in help_text:
            overlay = put_text_auto(overlay, text, (50, y), 21, (255, 255, 255))
            y += 35

        # åŠé€æ˜æ•ˆæœ
        cv.addWeighted(img, 0.3, overlay, 0.7, 0, img)

    def execute_custom_action(self, action):
        """æ‰§è¡Œè‡ªå®šä¹‰åŠ¨ä½œï¼ŒåŒ…æ‹¬æ¼”è®²ç¨¿ç›¸å…³åŠŸèƒ½"""
        try:
            if hasattr(action, 'value'):
                action_str = action.value
            else:
                action_str = str(action)
            
            # å¤„ç†æ¼”è®²ç¨¿ç›¸å…³åŠ¨ä½œ
            if action_str == "speech_scroll_toggle":
                self.toggle_speech_scroll()
            elif action_str == "speech_next":
                self.speech_next_segment()
            elif action_str == "speech_prev":
                self.speech_prev_segment()
            else:
                # å…¶ä»–åŠ¨ä½œäº¤ç»™PPTæ§åˆ¶å™¨å¤„ç†
                self.ppt_controller.execute_action(action)
                
        except Exception as e:
            print("æ‰§è¡Œè‡ªå®šä¹‰åŠ¨ä½œæ—¶å‡ºé”™:", e)

    def toggle_speech_scroll(self):
        """åˆ‡æ¢æ¼”è®²ç¨¿æ»šåŠ¨æ˜¾ç¤º"""
        self.show_speech_scroll = not self.show_speech_scroll
        
        if self.show_speech_scroll:
            if self.speech_display is None:
                self.speech_display = SpeechScrollDisplay(self.speech_manager)
            # print(            # åœ¨æ–°çº¿ç¨‹ä¸­å¯åŠ¨æ˜¾ç¤ºçª—å£
            import threading
            display_thread = threading.Thread(target=self.speech_display.show_display, daemon=True)
            display_thread.start()
        else:
            # print(
            if self.speech_display:
                cv.destroyWindow(self.speech_display.display_window_name)
    
    def speech_next_segment(self):
        """æ¼”è®²ç¨¿ä¸‹ä¸€æ®µ"""
        if self.speech_manager.manually_navigate("next"):
            current_slide = self.speech_manager.get_current_slide_number()
            # print(
            # å¯ä»¥é€‰æ‹©è‡ªåŠ¨åˆ‡æ¢å¹»ç¯ç‰‡
            # self.ppt_controller.jump_to_slide(current_slide)
    
    def speech_prev_segment(self):
        """æ¼”è®²ç¨¿ä¸Šä¸€æ®µ"""
        if self.speech_manager.manually_navigate("prev"):
            current_slide = self.speech_manager.get_current_slide_number()
            # print(
            # å¯ä»¥é€‰æ‹©è‡ªåŠ¨åˆ‡æ¢å¹»ç¯ç‰‡
            # self.ppt_controller.jump_to_slide(current_slide)
    
    def match_speech_text(self, input_text: str):
        """æ ¹æ®è¾“å…¥æ–‡æœ¬åŒ¹é…æ¼”è®²ç¨¿"""
        if self.speech_manager.match_input_text(input_text):
            current_slide = self.speech_manager.get_current_slide_number()
            # print(
            # å¦‚æœå¯ç”¨è‡ªåŠ¨æ»šåŠ¨ï¼Œå¯ä»¥è‡ªåŠ¨åˆ‡æ¢å¹»ç¯ç‰‡
            if self.speech_manager.auto_scroll_enabled:
                self.ppt_controller.jump_to_slide(current_slide)
    
    def handle_text_input(self):
        """å¤„ç†æ–‡æœ¬è¾“å…¥åŒ¹é…"""
        print("\nğŸ¤ è¯­éŸ³æ–‡æœ¬åŒ¹é…æ¨¡å¼")
        print("è¯·è¾“å…¥è¦åŒ¹é…çš„æ–‡æœ¬ (æŒ‰å›è½¦ç¡®è®¤ï¼Œè¾“å…¥'exit'é€€å‡º):")
        
        try:
            # æš‚åœOpenCVçª—å£çš„é”®ç›˜ç›‘å¬
            cv.destroyWindow(self.window_name)
            
            while True:
                user_input = input("è¾“å…¥æ–‡æœ¬> ").strip()
                
                if user_input.lower() == 'exit':
                    print("é€€å‡ºæ–‡æœ¬è¾“å…¥æ¨¡å¼")
                    break
                elif not user_input:
                    print("è¯·è¾“å…¥æœ‰æ•ˆæ–‡æœ¬")
                    continue
                
                # æ‰§è¡Œæ–‡æœ¬åŒ¹é…
                # print(
                self.match_speech_text(user_input)
                
                # æ˜¾ç¤ºå½“å‰æ¼”è®²ç¨¿çŠ¶æ€
                current_segment = self.speech_manager.segments[self.speech_manager.current_index]
                # print(
                
        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­æ–‡æœ¬è¾“å…¥")
        except Exception as e:
            print("å¤„ç†æ–‡æœ¬è¾“å…¥æ—¶å‡ºé”™:", e)
        
        # é‡æ–°åˆ›å»ºOpenCVçª—å£
        print("è¿”å›æ‰‹åŠ¿è¯†åˆ«æ¨¡å¼...")
        print("æŒ‰ 'T' å†æ¬¡è¿›å…¥æ–‡æœ¬è¾“å…¥æ¨¡å¼")

    def run(self):
        """è¿è¡Œä¸»ç¨‹åº"""
        print(" ç»Ÿä¸€PPTæ‰‹åŠ¿è¯†åˆ«æ’­æ”¾å™¨å¯åŠ¨")
        print(" æ”¯æŒçš„æ‰‹åŠ¿:")
        for key, config in self.gesture_configs.items():
            if config.enabled:  # åªæ˜¾ç¤ºå¯ç”¨çš„æ‰‹åŠ¿
                print(f"  - {config.name}: {config.gesture_type.value}")
        print(" æŒ‰ 'H' æ˜¾ç¤ºå¸®åŠ©ï¼ŒæŒ‰ 'Q' é€€å‡º")
        print("=" * 60)

        cap = cv.VideoCapture(0)
        pTime = 0

        # è®¾ç½®æ‘„åƒå¤´å‚æ•°
        cap.set(cv.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv.CAP_PROP_FRAME_HEIGHT, 480)
        cap.set(cv.CAP_PROP_FPS, 30)
        
        # åˆ›å»ºçª—å£ï¼Œä½¿ç”¨è‹±æ–‡åç§°é¿å…ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
        cv.namedWindow(self.window_name, cv.WINDOW_NORMAL)
        # è®¾ç½®çª—å£ä½ç½®å’Œå¤§å°
        cv.resizeWindow(self.window_name, 640, 480)
        cv.moveWindow(self.window_name, 100, 100)
        
        # å°è¯•è®¾ç½®çª—å£ä¸ºæ— è¾¹æ¡†ï¼ˆéƒ¨åˆ†ç³»ç»Ÿæ”¯æŒï¼‰
        try:
            # ä½¿ç”¨Qtåç«¯æ—¶å¯ä»¥è®¾ç½®æ— è¾¹æ¡†
            cv.setWindowProperty(self.window_name, cv.WND_PROP_ASPECT_RATIO, cv.WINDOW_FREERATIO)
        except:
            pass

        while self.running:
            success, img = cap.read()
            if not success:
                print(" æ— æ³•è¯»å–æ‘„åƒå¤´")
                break

            # å¤„ç†å½“å‰å¸§
            img = self.process_frame(img)

            # è®¡ç®—FPS
            cTime = time.time()
            if cTime != pTime:
                self.fps = 1 / (cTime - pTime)
            pTime = cTime
            self.frame_count += 1            # æ˜¾ç¤ºå›¾åƒ
            cv.imshow(self.window_name, img)
            
            # ç¡®ä¿æ‘„åƒå¤´çª—å£å§‹ç»ˆåœ¨æœ€å‰é¢
            cv.setWindowProperty(self.window_name, cv.WND_PROP_TOPMOST, 1)# å¤„ç†æŒ‰é”®
            key = cv.waitKey(1) & 0xFF
            if key == ord('q') or key == 27:  # Qæˆ–ESCé€€å‡º
                self.running = False
            elif key == ord('h'):  # Hæ˜¾ç¤º/éšè—å¸®åŠ©
                self.show_help = not self.show_help
            elif key == ord('c'):  # Cæ ¡å‡†æ¨¡å¼
                self.calibration_mode = not self.calibration_mode
                print(f"æ ¡å‡†æ¨¡å¼: {'å¼€å¯' if self.calibration_mode else 'å…³é—­'}")
            elif key == ord('s'):  # Sä¿å­˜é…ç½®
                self.save_gesture_configs()
            elif key == ord('t'):  # Té”®è§¦å‘æ–‡æœ¬è¾“å…¥åŒ¹é…
                self.handle_text_input()
            elif key == ord('n'):  # Né”®æ¼”è®²ç¨¿ä¸‹ä¸€æ®µ
                self.speech_next_segment()
            elif key == ord('p'):  # Pé”®æ¼”è®²ç¨¿ä¸Šä¸€æ®µ
                self.speech_prev_segment()
            elif key == ord('+') or key == ord('='):  # +é”®å¢åŠ å†·å´æ—¶é—´
                self.command_cooldown_duration = min(5.0, self.command_cooldown_duration + 0.5)
                print(f"å‘½ä»¤å†·å´æ—¶é—´å¢åŠ åˆ°: {self.command_cooldown_duration}ç§’")
            elif key == ord('-'):  # -é”®å‡å°‘å†·å´æ—¶é—´
                self.command_cooldown_duration = max(0.5, self.command_cooldown_duration - 0.5)
                print(f"å‘½ä»¤å†·å´æ—¶é—´å‡å°‘åˆ°: {self.command_cooldown_duration}ç§’")

        # æ¸…ç†èµ„æº
        cap.release()
        cv.destroyAllWindows()

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        total_time = time.time() - self.start_time
        avg_fps = self.frame_count / total_time if total_time > 0 else 0
        print(f"\nç¨‹åºè¿è¡Œç»Ÿè®¡:")
        print(f"   æ€»è¿è¡Œæ—¶é—´: {total_time:.1f}ç§’")
        print(f"   å¤„ç†å¸§æ•°: {self.frame_count}")
        print(f"   å¹³å‡FPS: {avg_fps:.1f}")
        print("ç»Ÿä¸€PPTæ‰‹åŠ¿è¯†åˆ«æ’­æ”¾å™¨å·²é€€å‡º")


def main():
    """ä¸»å‡½æ•°"""
    try:
        controller = UnifiedPPTGestureController()
        controller.run()
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f" ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
